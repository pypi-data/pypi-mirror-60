{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aalPefrUUplk"
      },
      "source": [
        "# Fairness Indicators on TF-Hub Text Embeddings\n",
        "\n",
        "In this colab, you will learn how to use [Fairness Indicators](https://github.com/tensorflow/fairness-indicators) to evaluate embeddings from [TF Hub](https://www.tensorflow.org/hub). Fairness Indicators is a suite of tools that facilitates evaluation and visualization of fairness metrics on machine learning models. Fairness Indicators is built on top of [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma), TensorFlow's official model evaluation library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u33JXdluZ2lG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BAUEkqYlzP3W"
      },
      "outputs": [],
      "source": [
        "!pip install fairness-indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "B8dlyTyiTe-9"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import os\n",
        "import tempfile\n",
        "import apache_beam as beam\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        
        "import tensorflow_hub as hub\n",
        "import tensorflow_model_analysis as tfma\n",
        "from tensorflow_model_analysis.addons.fairness.view import widget_view\n",
        "from tensorflow_model_analysis.addons.fairness.post_export_metrics import fairness_indicators\n",
        "from fairness_indicators.examples import util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ekzb7vVnPCc"
      },
      "source": [
        "# Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n4_nXQDykX6W"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = tempfile.gettempdir()\n",
        "\n",
        "# The input and output features of the classifier\n",
        "TEXT_FEATURE = 'comment_text'\n",
        "LABEL = 'toxicity'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xz4PcI0hSVcq"
      },
      "source": [
        "# Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Fg0JsKFSrsj"
      },
      "source": [
        "In this exercise, we'll work with the [Civil Comments dataset](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification), approximately 2 million public comments made public by the [Civil Comments platform](https://github.com/reaktivstudios/civil-comments) in 2017 for ongoing research. This effort was sponsored by Jigsaw, who have hosted competitions on Kaggle to help classify toxic comments as well as minimize unintended model bias.\n",
        "\n",
        "Each individual text comment in the dataset has a toxicity label, with the label being 1 if the comment is toxic and 0 if the comment is non-toxic. Within the data, a subset of comments are labeled with a variety of identity attributes, including categories for gender, sexual orientation, religion, and race or ethnicity.\n",
        "\n",
        "You can choose to download the original dataset and process it in the colab, which may take minutes, or you can download the preprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NUmSmqYGS0n8"
      },
      "outputs": [],
      "source": [
        "download_original_data = False\n",
        "\n",
        "if download_original_data:\n",
        "  train_tf_file = tf.keras.utils.get_file('train_tf.tfrecord',\n",
        "                                          'https://storage.googleapis.com/civil_comments_dataset/train_tf.tfrecord')\n",
        "  validate_tf_file = tf.keras.utils.get_file('validate_tf.tfrecord',\n",
        "                                             'https://storage.googleapis.com/civil_comments_dataset/validate_tf.tfrecord')\n",
        "\n",
        "  # The identity terms list will be grouped together by their categories\n",
        "  # on threshould 0.5. Only the identity term column, text column,\n",
        "  # and label column will be kept after processing.\n",
        "  train_tf_file = util.convert_comments_data(train_tf_file)\n",
        "  validate_tf_file = util.convert_comments_data(validate_tf_file)\n",
        "\n",
        "else:\n",
        "  train_tf_file = tf.keras.utils.get_file('train_tf_processed.tfrecord',\n",
        "                                          'https://storage.googleapis.com/civil_comments_dataset/train_tf_processed.tfrecord')\n",
        "  validate_tf_file = tf.keras.utils.get_file('validate_tf_processed.tfrecord',\n",
        "                                             'https://storage.googleapis.com/civil_comments_dataset/validate_tf_processed.tfrecord')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzcVLKPW7sjn"
      },
      "source": [
        "## Identity Terms\n",
        "\n",
        "You can select the subset of identity groups you are interested in by removing the others from the list below. By default, we will look at all identity terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "l88BZ7rFKi4-"
      },
      "outputs": [],
      "source": [
        "IDENTITY_TERMS = ['gender', 'sexual_orientation', 'race', 'religion', 'disability']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zz1NLR5Uu3oQ"
      },
      "source": [
        "# Creating a TensorFlow Model Analysis Pipeline\n",
        "\n",
        "The Fairness Indicators library operates on [TensorFlow Model Analysis (TFMA) models](https://www.tensorflow.org/tfx/model_analysis/get_started). TFMA models wrap [TensorFlow models](https://www.tensorflow.org/guide/estimator) with additional functionality to evaluate and visualize their results. The actual evaluation occurs inside of an [Apache Beam pipeline](https://beam.apache.org/documentation/programming-guide/).\n",
        "\n",
        "So we need to...\n",
        "1. Build a TensorFlow model.\n",
        "2. Build a TFMA model on top of the TensorFlow model.\n",
        "3. Run the model analysis in a Beam pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOUjSzEvxO81"
      },
      "source": [
        "## 1) Build a TensorFlow Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5dIo0yXvQdjo"
      },
      "source": [
        "### Define an Input Function\n",
        "\n",
        "TensorFlow parses features from data using [`FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature) and [`VarLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/VarLenFeature). So to allow TensorFlow to parse our data, we will need to map out our input feature, output feature, and any slicing features that we will want to analyze via Fairness Indicators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bFhj0N12SnWv"
      },
      "outputs": [],
      "source": [
        "FEATURE_MAP = {\n",
        "    # input and output features\n",
        "    LABEL: tf.io.FixedLenFeature([], tf.float32),\n",
        "    TEXT_FEATURE: tf.io.FixedLenFeature([], tf.string),\n",
        "\n",
        "    # slicing features\n",
        "    'sexual_orientation': tf.io.VarLenFeature(tf.string),\n",
        "    'gender': tf.io.VarLenFeature(tf.string),\n",
        "    'religion': tf.io.VarLenFeature(tf.string),\n",
        "    'race': tf.io.VarLenFeature(tf.string),\n",
        "    'disability': tf.io.VarLenFeature(tf.string)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W97S4JhASwe_"
      },
      "source": [
        "Now that we have defined our features and their types, we can create an input function for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3IbTVvhAQgmq"
      },
      "outputs": [],
      "source": [
        "def input_fn(tf_file):\n",
        "  def parse_function(serialized):\n",
        "    parsed_example = tf.io.parse_single_example(\n",
        "        serialized=serialized, features=FEATURE_MAP)\n",
        "    # Adds a weight column to deal with unbalanced classes.\n",
        "    parsed_example['weight'] = tf.add(parsed_example[LABEL], 0.1)\n",
        "    return (parsed_example,\n",
        "            parsed_example[LABEL])\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      filenames=[tf_file]).map(parse_function).batch(512)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "23i7M_w_bvQu"
      },
      "source": [
        "### Train a Classifier\n",
        "\n",
        "For each text embedding, we will train a **[DNN Classifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier)**.\n",
        "\n",
        "**TF Hub** allows us to insert text embeddings as features to our model via **[`text_embedding_column`](https://www.tensorflow.org/hub/api_docs/python/hub/text_embedding_column)**. The function's signature is **`text_embedding_column(key, module_spec)`**, where...\n",
        "\n",
        "* *`key`* is the name of the DataFrame's text feature (ex: `\"comment_text\"`)\n",
        "* *`module_spec`* is a url path to an text embedding module (ex: `\"https://tfhub.dev/google/nnlm-en-dim128/1\"`)\n",
        "\n",
        "Because each text embedding column is memory-intensive, the Colaboratory environment may crash if all embeddings are loaded at once. To avoid this, we encapsulate the embedding columns inside a pipeline and wait to get the pipeline's results before loading the next embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2NJHroV8CrdL"
      },
      "outputs": [],
      "source": [
        "def train_classifier(embedding):\n",
        "  embedded_text_feature_column = hub.text_embedding_column(\n",
        "      key=TEXT_FEATURE, \n",
        "      module_spec=embedding)\n",
        "  model_dir = os.path.join(BASE_DIR, 'train', datetime.now().strftime(\n",
        "    \"%Y%m%d-%H%M%S\"))\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "      hidden_units=[500, 100],\n",
        "      weight_column='weight',\n",
        "      feature_columns=[embedded_text_feature_column],\n",
        "      n_classes=2,\n",
        "      optimizer=tf.optimizers.Adagrad(learning_rate=0.003),\n",
        "      loss_reduction=tf.losses.Reduction.SUM,\n",
        "      model_dir= model_dir)\n",
        "  classifier.train(input_fn=lambda: input_fn(train_tf_file), steps=1000);\n",
        "  return classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nmi2aniNxmfo"
      },
      "source": [
        "## 2) Build a TFMA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hnoxsKIDXAOY"
      },
      "source": [
        "TFMA represents datasets as [`tf.Examples`](https://www.tensorflow.org/tutorials/load_data/tfrecord#tfexample), which it parses with [`EvalInputReceivers`](https://github.com/tensorflow/model-analysis/blob/master/tensorflow_model_analysis/eval_saved_model/export.py#L42). Refer to [Getting Started with TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started#modify_an_existing_model) for more info on creating `EvalInputReceivers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DU6BEc41xuHs"
      },
      "outputs": [],
      "source": [
        "def eval_input_receiver_fn():\n",
        "  \"\"\"Create a tfma.export.EvalInputReceiver to parse input features.\"\"\"\n",
        "  serialized_tf_example = tf.compat.v1.placeholder(\n",
        "      dtype=tf.string, shape=[None], name='input_example_placeholder')\n",
        "  receiver_tensors = {'examples': serialized_tf_example}\n",
        "  features = tf.io.parse_example(serialized_tf_example, FEATURE_MAP)\n",
        "  features['weight'] = tf.ones_like(features[LABEL])\n",
        "  return tfma.export.EvalInputReceiver(\n",
        "    features=features,\n",
        "    receiver_tensors=receiver_tensors,\n",
        "    labels=features[LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0W5VYoVdW9Tp"
      },
      "source": [
        "TFMA represents models with its **[`EvalSharedModel`](https://github.com/tensorflow/model-analysis/blob/master/tensorflow_model_analysis/types.py#L172)** class, which accepts a list of metrics to evaluate and visualize. TFMA represents metrics as callbacks which are computed after the model is exported - hence, the name **[`post_export_metrics`](https://github.com/tensorflow/model-analysis/blob/master/tensorflow_model_analysis/post_export_metrics/post_export_metrics.py)**. The metric provided by the Fairness Indicators library is **`post_export_metrics.fairness_indicators`**.\n",
        "\n",
        "`EvalSharedModel` is actually a thin wrapper around TFMA's **[`EvalSavedModel`](https://github.com/tensorflow/model-analysis/blob/master/tensorflow_model_analysis/eval_saved_model/load.py#L54)** class. To create an `EvalSavedModel`, we need to pass in a TensorFlow model and an `EvalInputReceiver`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I67Zm4Dc9UE2"
      },
      "outputs": [],
      "source": [
        "def create_tfma_model(classifier, eval_input_receiver_fn, metric_callbacks):\n",
        "\n",
        "  # create EvalSavedModel\n",
        "  eval_saved_model_path = tfma.export.export_eval_savedmodel(\n",
        "      estimator=classifier,\n",
        "      export_dir_base=os.path.join(BASE_DIR, 'tfma_eval_model'),\n",
        "      eval_input_receiver_fn=eval_input_receiver_fn)\n",
        "\n",
        "  # create EvalSharedModel\n",
        "  return tfma.default_eval_shared_model(\n",
        "        eval_saved_model_path=eval_saved_model_path,\n",
        "        add_metrics_callbacks=metric_callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1HbCbLLbWZLT"
      },
      "source": [
        "## 3) Get Evaluation Results in Apache Beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "geCqkTyMW5Nr"
      },
      "source": [
        "Our [Model Evaluation pipeline](https://www.tensorflow.org/tfx/model_analysis/get_started) will have two steps. The first step is to read in the data in a TF-compatible format - we can use [`beam.io.ReadFromTFRecord`](https://beam.apache.org/releases/pydoc/2.15.0/apache_beam.io.tfrecordio.html#apache_beam.io.tfrecordio.ReadFromTFRecord) for that.\n",
        "\n",
        "The second step is to evaluate the TFMA results. We use TFMA's [`ExtractEvaluateAndWriteResults`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ExtractEvaluateAndWriteResults) API, a [`PTransform`](https://beam.apache.org/documentation/programming-guide/#transforms) that takes in an `EvalSharedModel`, computes metrics for the slices specified in `slice_spec`, and writes them to an `output_path`.\n",
        "\n",
        "[`slice_spec`](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic#slicing_and_dicing) is how TFMA decides how to group the data. In this case study, the slices refer to different identity groups. We'll show you how to create a `slice_spec` in the next section.\n",
        "\n",
        "Check the [Get Started with TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started) tutorial for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZKh1Ale0KynF"
      },
      "outputs": [],
      "source": [
        "def get_eval_result(input_file, eval_shared_model,\n",
        "                    slice_spec, eval_result_path):\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    _ = (\n",
        "        pipeline\n",
        "        | 'ReadFromTFRecord' \u003e\u003e beam.io.ReadFromTFRecord(\n",
        "            file_pattern=input_file)\n",
        "        | 'ExtractEvaluateAndWriteResults' \u003e\u003e\n",
        "        tfma.ExtractEvaluateAndWriteResults(\n",
        "                  eval_shared_model=eval_shared_model,\n",
        "                  slice_spec=slice_spec,\n",
        "                  compute_confidence_intervals=False,\n",
        "                  output_path=eval_result_path)\n",
        "    )\n",
        "  return tfma.load_eval_result(output_path=eval_result_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wM-faceoCPqg"
      },
      "source": [
        "# Putting it all Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7nSvu4IUCigW"
      },
      "outputs": [],
      "source": [
        "def embedding_fairness_result(embedding):\n",
        "\n",
        "  # First, we use our train_classifier() function to train a basic classifier\n",
        "  # with our chosen embedding.\n",
        "  print(\"Training classifier for \" + embedding)\n",
        "  classifier = train_classifier(embedding)\n",
        "\n",
        "  # Next, we measure the accuracy of our classifier on our validation set.\n",
        "  train_eval_result = classifier.evaluate(input_fn=lambda: input_fn(validate_tf_file))\n",
        "  print('Validation set accuracy for {}: {accuracy}'.format(embedding, **train_eval_result))\n",
        "\n",
        "  # We then create a fairness_indicators callback to use in our TFMA model.\n",
        "  # `labels_key` is the target feature (\"toxicity\" in our case).\n",
        "  # `thresholds` are the values of the target feature at which to measure fairness metrics.\n",
        "  fairness_indicator_callback = tfma.post_export_metrics.fairness_indicators(\n",
        "                                    thresholds=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "                                    labels_key=LABEL)\n",
        "\n",
        "  # We then use our create_tfma_model() function, which converts our classifier\n",
        "  # to a TFMA EvalSharedModel that outputs Fairness Indicators.\n",
        "  eval_shared_model = create_tfma_model(classifier, eval_input_receiver_fn,\n",
        "                                        [fairness_indicator_callback])\n",
        "\n",
        "  # We select the slices we want to compute Fairness results for.\n",
        "  # In this case, we use the same identity terms that you selected at the\n",
        "  # beginning of the colab.\n",
        "  slice_spec = [tfma.slicer.SingleSliceSpec()]\n",
        "  for identity in IDENTITY_TERMS:\n",
        "    slice_spec.append(tfma.slicer.SingleSliceSpec(columns=[identity]))\n",
        "\n",
        "  # We also need to create a unique path to store our results for this embedding.\n",
        "  embedding_name = embedding.split('/')[-2]\n",
        "  eval_result_path = os.path.join(BASE_DIR, 'eval_result', embedding_name)\n",
        "\n",
        "  # Finally, we use our get_eval_result() function to compute and return the\n",
        "  # Fairness Indicators results!\n",
        "  eval_result = get_eval_result(validate_tf_file, eval_shared_model, slice_spec, eval_result_path)\n",
        "  return eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jTPqije9Eg5b"
      },
      "source": [
        "# Run TFMA \u0026 Fairness Indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8AvInTNt8Gyn"
      },
      "source": [
        "## Fairness Indicators Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jiLg5ikCzFR-"
      },
      "source": [
        "Refer [here](https://github.com/tensorflow/fairness-indicators) for more information on Fairness Indicators. Below are some of the available metrics.\n",
        "\n",
        "* [Negative Rate, False Negative Rate (FNR), and True Negative Rate (TNR)](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_and_false_negative_rates)\n",
        "* [Positive Rate, False Positive Rate (FPR), and True Positive Rate (TPR)](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_and_false_negative_rates)\n",
        "* [Accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy)\n",
        "* [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
        "* [Precision-Recall AUC](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC)\n",
        "* [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LGXCFtScblYt"
      },
      "source": [
        "## Text Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1CI-1M5qXGjG"
      },
      "source": [
        "**[TF-Hub](https://www.tensorflow.org/hub)** provides several **text embeddings**. These embeddings will serve as the feature column for our different models. For this Colab, we use the following embeddings:\n",
        "\n",
        "* [**random-nnlm-en-dim128**](https://tfhub.dev/google/random-nnlm-en-dim128/1): random text embeddings, this serves as a convenient baseline.\n",
        "* [**nnlm-en-dim128**](https://tfhub.dev/google/nnlm-en-dim128/1): a text embedding based on [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). \n",
        "* [**universal-sentence-encoder**](https://tfhub.dev/google/universal-sentence-encoder/2): a text embedding based on [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xxq97Qt7itVL"
      },
      "source": [
        "## Fairness Indicator Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "27FX15awixuK"
      },
      "source": [
        "For each of the above embeddings, we will compute fairness indicators with our `embedding_fairness_result` pipeline, and then render the results in the Fairness Indicator UI widget with `widget_view.render_fairness_indicator`.\n",
        "\n",
        "Note that the `widget_view.render_fairness_indicator` cells may need to be run twice for the visualization to be displayed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yEUbZ93y8NCW"
      },
      "source": [
        "#### Random NNLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DkSuox-Pb6Pz"
      },
      "outputs": [],
      "source": [
        "eval_result_random_nnlm = embedding_fairness_result('https://tfhub.dev/google/random-nnlm-en-dim128/1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "05xUesz6VpAe"
      },
      "outputs": [],
      "source": [
        "widget_view.render_fairness_indicator(eval_result_random_nnlm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jmKe8Z1b8SBy"
      },
      "source": [
        "##### NNLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5b8HcTUBckj1"
      },
      "outputs": [],
      "source": [
        
        "eval_result_nnlm = embedding_fairness_result('https://tfhub.dev/google/nnlm-en-dim128/1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n6hasLzFVrDN"
      },
      "outputs": [],
      "source": [
        
        "widget_view.render_fairness_indicator(eval_result_nnlm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1I4xEDNq8T0X"
      },
      "source": [
        "##### Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GrdweWRkck8A"
      },
      "outputs": [],
      "source": [
        
        "eval_result_use = embedding_fairness_result('https://tfhub.dev/google/universal-sentence-encoder/2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JBABAkZMVtTK"
      },
      "outputs": [],
      "source": [
        
        "widget_view.render_fairness_indicator(eval_result_use)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O2AnFO6Hcofi"
      },
      "source": [
        "## Exercises\n",
        "1. Pick an identity category, such as religion or sexual orientation, and look at False Positive Rate for the Universal Sentence Encoder. How do different slices compare to each other? How do they compare to the Overall baseline?\n",
        "2. Now pick a different identity category. Compare the results of this category with the previous one. Does the model weigh one category as more \"toxic\" than the other? Does this change with the embedding used?\n",
        "3. Does the model generally tend to overestimate or underestimate the number of toxic comments?\n",
        "4. Look at the graphs for different fairness metrics. Which metrics seem most informative? Which embeddings perform best and worst for that metric?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "Fairness Indicators on TF-Hub Text Embeddings",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
