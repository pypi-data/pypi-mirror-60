#! coding: utf-8
import asyncio
import json
import os
import pathlib
import re
import time
from functools import partial
from inspect import getsource
from threading import Lock
from traceback import format_exc

import requests
from torequests.dummy import Requests
from torequests.logs import init_logger
from torequests.utils import (curlparse, find_one, flush_print, md5, ttime,
                              unique)

# 140 like weibo
SHORTEN_RESULT_MAX_LENGTH = 140
GLOBAL_LOCK = Lock()


def _default_shorten_result_function(result):
    string = str(result)
    if len(string) < SHORTEN_RESULT_MAX_LENGTH:
        return string.strip()
    else:
        # 32bit md5
        return md5(result)


class WatchdogTask(object):
    logger = init_logger('WatchdogTask')
    req = None
    # frequency format: (concurrent_count, interval)
    DEFAULT_HOST_FREQUENCY = (1, 1)
    CHROME_UA = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'
    # reset the default `shorten_result_function`
    BeautifulSoup = None
    BeautifulSoupFeatures = 'html.parser'
    Tree = None
    GLOBAL_TIMEOUT = 10
    GLOBAL_RETRY = 3

    def __init__(self,
                 name,
                 request_args,
                 parser_name=None,
                 operation=None,
                 value=None,
                 tag='',
                 work_hours='0, 24',
                 sorting_list=True,
                 check_interval=300,
                 last_check_time=None,
                 max_change=1,
                 check_result_list=None,
                 origin_url=None,
                 encoding=None,
                 last_change_time=None,
                 enable=True,
                 custom='',
                 **nonsense_kwargs):
        """Watchdog task.
            :param name: Task name.
            :type name: str
            :param request_args: arg for sending a request, could be url/curl_string/dict.
            :type request_args: dict / str
            :param parser_name: re, css, json, python, defaults to None, use the resp.text.
            :type parser_name: str, optional
            :param operation: parse operation for the parser_name, defaults to None
            :type operation: str, optional
            :param value: value operation for the parser, defaults to None
            :type value: str, optional
            :param tag: tag for filter, split by "/", defaults to "default"
            :type tag: str, optional
            :param work_hours: work_hours of the crawler, defaults to '0, 24', means range(0, 24), or [1, 3, 5] json-like string
            :type work_hours: str, optional
            :param sorting_list: whether sorting the list of result from `css or other parsers`, defaults to True
            :type sorting_list: bool, optional
            :param check_interval: check_interval, defaults to 300 seconds
            :type check_interval: int, optional
            :param last_check_time: last checking ttime like 2019-08-23 19:29:14, defaults to None
            :type last_check_time: str, optional
            :param max_change: save result in check_result_list, save the latest 5 change, defaults to 5
            :type max_change: list, optional
            :param check_result_list: latest `max_change` checking result, usually use md5 to shorten it, defaults to None
            :type check_result_list: list, optional
            :param origin_url: the url to see the changement, defaults to request_args['url']
            :type origin_url: str, optional
            :param encoding: http response encoding
            :type encoding: str
            :param last_change_time: last change time
            :type last_change_time: str, optional
            :param enable: skip crawl if False
            :type enable: bool, optional
            :param custom: some string for callback or notification
            :type custom: bool, optional

            request_args examples:
                url:
                    http://pypi.org
                args:
                    {'url': 'http://pypi.org', 'method': 'get'}
                curl:
                    curl 'https://pypi.org/' -H 'authority: pypi.org' -H 'cache-control: max-age=0' -H 'upgrade-insecure-requests: 1' -H 'user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36' -H 'sec-fetch-mode: navigate' -H 'sec-fetch-user: ?1' -H 'dnt: 1' -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3' -H 'sec-fetch-site: none' -H 'accept-encoding: gzip, deflate, br' -H 'accept-language: zh-CN,zh;q=0.9' -H 'cookie: user_id__insecure=; session_id=' --compressed

            parser examples:
                re:
                    operation = '.*?abc'
                    value = '$0' (or '$1', `$` means the group index for regex result)
                css:
                    operation = ".className"
                    value = '$string'
                        $string: return [node] as outer html
                        $text: return [node.text]
                        $get_text: return [node.get_text()]
                        @attr: [get attribute from node]
                json:
                    view more: https://github.com/adriank/ObjectPath
                    # input response JSON string: {"a": 1}
                    operation = "$.a"
                    value = None

                python:
                    ! function name should always be `parse` if value is None,
                        or use `value` as the function name.
                    `operation can be a function object.`
                    operation = r'''
                    def parse(resp):
                        return md5(resp.text)
                    '''
                    value = None
        """
        self.name = str(name)
        self.request_args = self._ensure_request_args(request_args)
        self.parser_name = parser_name
        self.operation = operation
        self.value = value
        self.tag = self._ensure_tags(tag)
        self.work_hours = self.ensure_work_hours(work_hours)
        self.enable = bool(enable)
        self.custom = custom
        self.check_interval = int(check_interval)
        self.sorting_list = sorting_list
        self.last_check_time = last_check_time
        self.max_change = int(max_change)
        self.last_change_time = last_change_time or ttime(0)
        # check_result_list: [{'data': 'xxx', 'time': '2019-08-23 19:27:20'}]
        if isinstance(check_result_list, str):
            check_result_list = json.loads(check_result_list)
        self.check_result_list = check_result_list or []
        self.origin_url = origin_url or self.request_args['url']
        self.encoding = encoding or None
        self.update_last_change_time()
        self.ensure_req()

    @classmethod
    def ensure_req(cls):
        if not cls.req:
            cls.req = Requests(
                default_host_frequency=cls.DEFAULT_HOST_FREQUENCY)

    @classmethod
    def _ensure_tags(cls, tag):
        if not tag:
            return ['default']
        elif isinstance(tag, str):
            tags = [t.strip() for t in re.split(r'\s*,\s*', tag)]
            return [i for i in unique(tags) if i]
        else:
            # for list / set / tuple / dict
            return list(tag)

    @staticmethod
    def ensure_work_hours(work_hours):
        try:
            whs = json.loads(work_hours or "0, 24")
            for wh in whs:
                if not isinstance(wh, int):
                    break
            else:
                # good json-like int list
                return work_hours
            # bad json
            return '0, 24'
        except json.JSONDecodeError:
            work_hours = re.findall(r'\d+', work_hours or '0, 24')[:2]
            return ', '.join(work_hours)
        except Exception:
            return '0, 24'

    def check_work_hours(self, current_hour=None):
        current_hour = current_hour or int(time.strftime('%H'))
        if re.match(r'^\d+, \d+$', self.work_hours):
            return current_hour in range(*(
                int(i) for i in self.work_hours.split(', ')))
        else:
            return current_hour in json.loads(self.work_hours)

    @property
    def is_finished(self):
        return len(self.check_result_list) >= self.max_change

    def update_last_change_time(self):
        self.last_change_time = ttime(0)
        for item in self.check_result_list:
            if item['time'] > self.last_change_time:
                self.last_change_time = item['time']
        return self.last_change_time

    def _default_parser(self, resp):
        if resp:
            return resp.text
        else:
            self.logger.error(
                f'[{self.name}] request fail: [{getattr(resp, "status_code", -1)}], {resp.url}\n{resp.text.strip()[:200]} ...'
            )
            return ''

    def _re_parser(self, resp):
        if resp:
            scode = resp.content.decode(
                self.encoding, errors='ignore') if self.encoding else resp.text
            result = find_one(self.operation, scode)
            if not (isinstance(self.value, str) and self.value.startswith('$')):
                raise ValueError(
                    f'value should be string startswith `$`, like $1, $0, but {self.value} given.'
                )
            index = int(self.value[1:])
            return result[index]
        else:
            self.logger.error(
                f'[{self.name}] request fail: [{getattr(resp, "status_code", -1)}], {resp.url}\n{resp.text.strip()[:200]} ...'
            )
            return ''

    def _css_parser(self, resp):
        if not self.BeautifulSoup:
            from bs4 import BeautifulSoup
            self.__class__.BeautifulSoup = BeautifulSoup
        if resp:
            result = []
            scode = resp.content.decode(
                self.encoding, errors='ignore') if self.encoding else resp.text
            soup = self.BeautifulSoup(
                scode, features=self.BeautifulSoupFeatures)
            result = soup.select(self.operation)
            if self.value == '$text':
                result = [item.text for item in result]
            elif self.value == '$get_text':
                result = [item.get_text() for item in result]
            elif not self.value or self.value == '$string':
                result = [str(item) for item in result]
            elif self.value.startswith('@'):
                result = [item.get(self.value[1:], '') for item in result]
                # for class always be seen as list
                result = [
                    ' '.join(item) if isinstance(item, list) else item
                    for item in result
                ]
            # ensure the plain sequence
            if self.sorting_list:
                result = sorted([(item or '').strip() for item in result])
            return result
        else:
            self.logger.error(
                f'[{self.name}] request fail: [{getattr(resp, "status_code", -1)}], {resp.url}\n{resp.text.strip()[:200]} ...'
            )
            return ''

    def _python_parser(self, resp):
        if resp:
            if callable(self.operation):
                parse_function = self.operation
            else:
                if not isinstance(self.operation, str):
                    raise ValueError(
                        f'self.operation expect type str, but {type(self.operation)} given.'
                    )
                self.operation = self.operation.strip()
                exec(self.operation)
                function = locals().get(self.value or 'parse')
                if function:
                    self._python_parser_function = function
                else:
                    raise ValueError(
                        f'invalid function code from operation, function name should be parse: {self.operation}'
                    )
                parse_function = self._python_parser_function
            result = parse_function(resp)
            return result
        else:
            self.logger.error(
                f'[{self.name}] request fail: [{getattr(resp, "status_code", -1)}], {resp.url}\n{resp.text.strip()[:200]} ...'
            )
            return ''

    def _json_parser(self, resp):
        if resp:
            try:
                json_object = json.loads(resp.content)
            except json.JSONDecodeError:
                self.logger.error('')
                return ''
            if not self.Tree:
                from objectpath import Tree
                self.__class__.Tree = Tree
            tree = self.Tree(json_object)
            result = tree.execute(self.operation)
            return result
        else:
            self.logger.error(
                f'[{self.name}] request fail: [{getattr(resp, "status_code", -1)}], {resp.url}\n{resp.text.strip()[:200]} ...'
            )
            return ''

    def _ensure_parser(self, parser_name):
        parsers = {
            're': self._re_parser,
            'css': self._css_parser,
            'python': self._python_parser,
            'json': self._json_parser,
        }
        return parsers.get(parser_name, self._default_parser)

    def _ensure_request_args(self, request_args):
        if not request_args:
            raise ValueError('request_args should not be null')
        if isinstance(request_args, str):
            request_args = request_args.strip()
            if request_args.startswith('http'):
                return {
                    'url': request_args,
                    'method': 'get',
                    'headers': {
                        'User-Agent': self.CHROME_UA
                    }
                }
            elif request_args.startswith('curl'):
                return curlparse(request_args)
            elif request_args.startswith('{'):
                return json.loads(request_args)
            else:
                raise ValueError(
                    'request_args string should be a curl string or url')
        elif isinstance(request_args, dict):
            return request_args
        else:
            raise ValueError(
                f'please ensure your arg as str(startswith `http` or `curl`) / dict: {request_args}'
            )

    async def get_resp(self):
        resp = await self.req.request(
            retry=self.GLOBAL_RETRY,
            timeout=self.GLOBAL_TIMEOUT,
            verify=0,
            **self.request_args)
        return resp

    def get_parse_result(self, resp):
        parser = self._ensure_parser(self.parser_name)
        result = parser(resp)
        return result

    async def fetch_once(self):
        resp = await self.get_resp()
        if not resp:
            error = f'{self.name} request failed: {resp.text}'
            self.logger.error(error)
            raise requests.HTTPError(error)
        result = self.get_parse_result(resp)
        return result

    async def test(self):
        result = await self.fetch_once()
        return result

    def sync_test(self):
        resp = requests.request(verify=0, **self.request_args)
        result = self.get_parse_result(resp)
        return result

    def _ensure_function_code(self, func):
        if not func:
            return None
        if callable(func):
            return getsource(func)
        else:
            return str(func)

    def __str__(self):
        return f'<WatchdogTask {self.name}>'

    def to_json(self, **kwargs):
        return json.dumps(self.to_dict(), **kwargs)

    def to_dict(self):
        return {
            'name': self.name,
            'request_args': self.request_args,
            'parser_name': self.parser_name,
            'operation': self.operation,
            'value': self.value,
            'tag': self.tag,
            'work_hours': self.work_hours,
            'check_interval': self.check_interval,
            'sorting_list': self.sorting_list,
            'last_check_time': self.last_check_time,
            'check_result_list': self.check_result_list,
            'max_change': self.max_change,
            'origin_url': self.origin_url,
            'encoding': self.encoding,
            'last_change_time': self.last_change_time,
            'custom': self.custom,
            'enable': self.enable,
        }

    def dump_task(self):
        """Dump task info into JSON string.
        """
        return self.to_json()

    @classmethod
    def load_task(cls, json_or_dict):
        if isinstance(json_or_dict, str):
            json_or_dict = json.loads(json_or_dict)
        return cls(**json_or_dict)

    @classmethod
    def load_rss_task(cls, url):
        kwargs = {
            'name': url,
            'request_args': url,
            'parser_name': 'css',
            'value': '$text',
            'sorting_list': False,
            'origin_url': url,
            'max_change': 2,
        }
        try:
            r = requests.request(
                **{
                    'url': url,
                    'method': 'get',
                    'headers': {
                        'User-Agent': cls.CHROME_UA
                    },
                    'timeout': 5,
                })
            if not cls.BeautifulSoup:
                from bs4 import BeautifulSoup
                cls.BeautifulSoup = BeautifulSoup
            try:
                scode = r.content.decode('utf-8')
                kwargs['encoding'] = 'utf-8'
            except UnicodeDecodeError:
                scode = r.content.decode('gb18030')
                kwargs['encoding'] = 'gb18030'

            soup = cls.BeautifulSoup(scode, features=cls.BeautifulSoupFeatures)
            title = soup.select_one('title')
            if title and title.text.strip():
                kwargs['name'] = title.text.strip()
            match = re.search(
                '<link>(https?://[^<]+)</link>', scode) or re.search(
                    '<link [^>]*href=[\'"](https?://[^"\']+)[\'"]', scode)
            if match:
                kwargs['origin_url'] = match.group(1).strip()
            now = ttime()
            kwargs['last_check_time'] = now
            items = soup.select('item:first-of-type>title')
            if items:
                kwargs['operation'] = 'item:first-of-type>title'
            else:
                items = soup.select('entry:first-of-type>title')
                kwargs['operation'] = 'entry:first-of-type>title'
            kwargs['check_result_list'] = [{
                'data': i.text,
                'time': now,
            } for i in items][:kwargs['max_change']]
            return cls(**kwargs)
        except Exception as e:
            cls.logger.error(f'add rss failed:\n{format_exc()}')
            return e

    async def handle_change_custom(self):
        if not self.custom:
            return
        method, arg = self.custom.split(':', 1)
        function = getattr(self, f'_handle_{method}', None)
        if function:
            if asyncio.iscoroutinefunction(function):
                await function(arg)
            elif callable(function):
                await self.loop.run_in_executor(None, function, arg)

    async def _handle_serverjiang(self, arg):
        text = f'{self.name}#{self.last_change_time}'
        description: str = self.check_result_list[0][
            'data'] if self.check_result_list else ''
        body = f'{self.origin_url}\n\n{description}'
        url = f'https://sc.ftqq.com/{arg}.send'
        await self.req.post(url, data={'text': text, 'desp': body})


class WatchdogCage(object):
    logger = init_logger('WatchdogCage')

    def __init__(self,
                 file_path=None,
                 shorten_result_function=None,
                 auto_save=True,
                 loop_interval=300,
                 pretty_json=True,
                 change_callback=None,
                 loop=None,
                 quota=20):
        self.file_path = file_path or self.default_file_path
        # self.tasks: dict with key=task_name, value=WatchdogTask obj
        self.tasks = self.load_tasks_from_file(self.file_path)
        self.shorten_result_function = shorten_result_function or _default_shorten_result_function
        self.auto_save = auto_save
        self.loop_interval = loop_interval
        self.pretty_json = pretty_json
        self.change_callback = change_callback
        self.loop = loop
        self.quota = quota
        self._force_crawl = False

    def check_quota(self):
        if len(self.tasks) >= self.quota:
            raise IOError(
                f'The quota has been exceeded {len(self.tasks)} / {self.quota}.'
            )

    @classmethod
    def refresh_file_path(cls, file_path):
        cls.logger.info(f'refreshing json file {file_path}.')
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('{}')

    @classmethod
    def backup_file_path(cls, file_path):
        with open(f'{file_path}.backup', 'wb') as f_write, open(
                file_path, 'rb') as f_read:
            f_write.write(f_read.read())

    @property
    def default_file_path(self):
        dir_path = pathlib.Path(os.path.expanduser("~")) / "watch_dog_cage"
        file_path = dir_path / 'default_tasks.json'
        self.logger.info(f'using default_file_path: {file_path}')
        if not dir_path.is_dir():
            self.logger.warning(f'`{dir_path}` directory not found. mkdir...')
            dir_path.mkdir()
        if not (file_path.is_file() and file_path.stat().st_size):
            self.logger.warning(f'`{file_path}` is null, rewriting it.')
            self.refresh_file_path(file_path)
        return str(file_path)

    def check_auto_save(self):
        if self.auto_save:
            self.save_tasks()

    def add_task(self, task):
        self.check_quota()
        if task.name in self.tasks:
            self.logger.error(f'{task} has existed, please rename/remove it.')
            return False
        self.tasks[task.name] = task
        self.check_auto_save()
        return True

    def update_task(self, task):
        self.check_quota()
        ok = task.name in self.tasks
        self.tasks[task.name] = task
        self.check_auto_save()
        return ok

    def remove_task(self, task_name):
        is_exist = bool(self.tasks.pop(task_name, None))
        if is_exist:
            self.check_auto_save()
        return task_name not in self.tasks

    def get_task(self, task_name=None):
        if task_name:
            task = self.tasks.get(task_name)
            if task:
                return task.to_dict()
        else:
            return self.tasks_dict

    @property
    def tasks_dict(self):
        return {task.name: task.to_dict() for task in self.tasks.values()}

    def save_tasks(self, file_path=None):
        with GLOBAL_LOCK:
            file_path = file_path or self.file_path
            with open(file_path, 'w', encoding='utf-8') as f:
                if self.pretty_json:
                    json.dump(self.tasks_dict, f, ensure_ascii=False, indent=2)
                else:
                    json.dump(self.tasks_dict, f)

    @staticmethod
    def load_tasks_from_file(file_path):
        with GLOBAL_LOCK:
            with open(file_path, encoding='utf-8') as f:
                tasks_dict = {
                    task_name: WatchdogTask.load_task(task_json)
                    for task_name, task_json in json.load(f).items()
                }
            return tasks_dict

    async def run_task(self, task):
        try:
            result = await task.fetch_once()
        except requests.HTTPError as err:
            return err
        if result:
            shorten_result = self.shorten_result_function(result)
        else:
            shorten_result = 'No result.'
        exist_results = {item['data'] for item in task.check_result_list}
        # if shorten_result not exist, insert into check_result_list
        if shorten_result not in exist_results:
            task.check_result_list.insert(0, {
                'data': shorten_result,
                'time': ttime()
            })
            state = f'task [{task.name}] has new change [{len(task.check_result_list)}/{task.max_change}] => `{shorten_result}`'
            task.update_last_change_time()
            if self.change_callback:
                if asyncio.iscoroutinefunction(self.change_callback):
                    await self.change_callback(task)
                elif callable(self.change_callback):
                    await self.loop.run_in_executor(None, self.change_callback,
                                                    task)
                else:
                    self.logger.error(
                        f'bad change_callback type: {type(self.change_callback)}, should be function/coroutinefunction'
                    )
            elif task.custom:
                # use default callback while custom is not null
                await task.handle_change_custom()

        else:
            state = ''
        task.check_result_list = task.check_result_list[:task.max_change]
        task.last_check_time = ttime()
        return state

    async def run(self):
        self.logger.info(f'{len(self.tasks)} tasks start running.')
        WatchdogTask.ensure_req()
        while 1:
            changes = []
            ttime_0 = ttime(0)
            if self._force_crawl:
                for task in self.tasks.values():
                    task.last_check_time = '2000-01-01 00:00:00'
                self._force_crawl = False
            current_hour = int(time.strftime('%H'))
            running_tasks = [
                asyncio.ensure_future(self.run_task(task))
                for task in self.tasks.values()
                if task.enable and task.check_work_hours(
                    current_hour=current_hour) and
                ttime(time.time() - task.check_interval) >
                (task.last_check_time or ttime_0)
            ]
            self.logger.info(
                f'{len(running_tasks)} tasks checking for interval overdue.')
            for task in running_tasks:
                state = await task
                changes.append(state)
            self.save_tasks()
            for change in changes:
                if change:
                    self.logger.info(change)
            for _ in range(int(self.loop_interval) + 1, 0, -1):
                await asyncio.sleep(1)
                flush_print(_, sep="", end=" ")
                if self._force_crawl:
                    self.logger.info('crawl for force_crawl')
                    break
            flush_print()

        self.logger.info('no tasks remaining.')

    def __del__(self):
        self.logger.info('stop running.')

    def __str__(self):
        tasks_set = {str(task) for task in self.tasks.values()}
        return str(tasks_set)


class WebHandler(object):
    logger = init_logger('WebHandler')
    VUE_JS_CDN = 'https://cdn.staticfile.org/vue/2.6.10/vue.min.js'
    ELEMENT_CSS_CDN = 'https://cdn.staticfile.org/element-ui/2.11.1/theme-chalk/index.css'
    ELEMENT_JS_CDN = 'https://cdn.staticfile.org/element-ui/2.11.1/index.js'
    VUE_RESOURCE_CDN = 'https://cdn.staticfile.org/vue-resource/1.5.1/vue-resource.min.js'

    def __init__(self,
                 app,
                 file_path=None,
                 shorten_result_function=None,
                 auto_save=True,
                 loop_interval=300,
                 pretty_json=True,
                 auto_open_browser=True,
                 change_callback=None,
                 app_kwargs=None,
                 username='',
                 password=''):
        self.app_kwargs = app_kwargs or {}
        self.host = self.app_kwargs.get('host', '127.0.0.1')
        self.port = self.app_kwargs.get('port', 9988)
        self.loop = asyncio.get_event_loop()
        self.wc = WatchdogCage(
            file_path=file_path,
            shorten_result_function=shorten_result_function,
            auto_save=auto_save,
            loop_interval=loop_interval,
            pretty_json=pretty_json,
            change_callback=change_callback,
            loop=self.loop)
        self.wc.username = username
        self.wc.password = password
        app.wc = self.wc
        app.logger = self.logger
        app.loop_interval = loop_interval
        app.lock = GLOBAL_LOCK
        app.cdn_urls = dict(
            VUE_JS_CDN=self.VUE_JS_CDN,
            ELEMENT_CSS_CDN=self.ELEMENT_CSS_CDN,
            ELEMENT_JS_CDN=self.ELEMENT_JS_CDN,
            VUE_RESOURCE_CDN=self.VUE_RESOURCE_CDN,
        )
        app.pid = os.getpid()
        app.console_url = self.console_url
        self.app = app
        self.auto_open_browser = auto_open_browser

    @property
    def console_url(self):
        return f'http://{self.host}:{self.port}'

    async def run_server(self):
        # http://127.0.0.1:9988/

        self.logger.info(
            f'run_server with kwargs: {self.app_kwargs}, console_url: {self.console_url}'
        )
        self.loop.run_in_executor(None, partial(self.app.run,
                                                **self.app_kwargs))
        if self.auto_open_browser:
            import webbrowser
            webbrowser.open(self.console_url)

        await asyncio.ensure_future(self.wc.run())

    def run(self):
        self.loop.run_until_complete(self.run_server())
