{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T09:37:00.035438Z",
     "start_time": "2019-11-22T09:36:59.146009Z"
    }
   },
   "outputs": [],
   "source": [
    "import sagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T09:47:38.252912Z",
     "start_time": "2019-11-22T09:47:38.230434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+\n",
      "|    | name   |   age |\n",
      "|----+--------+-------|\n",
      "|  0 | tom    |     5 |\n",
      "+----+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "sagas.print_rs([('tom',5)], ['name','age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T10:06:05.013367Z",
     "start_time": "2019-11-22T10:06:05.002028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpacyNLP',\n",
       " 'SpacyTokenizer',\n",
       " 'SpacyEntityExtractor',\n",
       " 'DucklingHTTPExtractor',\n",
       " 'CRFEntityExtractor',\n",
       " 'SklearnIntentClassifier']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rasa.nlu import config\n",
    "conf=config.load('saai/sample_configs/config_crf_custom_features.yml')\n",
    "# conf.for_component('DucklingHTTPExtractor')\n",
    "conf.component_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T10:08:30.223691Z",
     "start_time": "2019-11-22T10:08:30.208396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T10:15:04.335508Z",
     "start_time": "2019-11-22T10:15:04.331629Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(conf.get('DucklingHTTPExtractor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T10:36:58.077056Z",
     "start_time": "2019-11-22T10:36:57.416585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0\n",
      "will 5\n",
      "be 10\n",
      "tokenized 13\n",
      "with 23\n",
      "case 28\n",
      "sensitive 33\n",
      "as 43\n",
      "default 46\n"
     ]
    }
   ],
   "source": [
    "from rasa.nlu.training_data import TrainingData, Message\n",
    "from rasa.nlu.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "\n",
    "def testing_tokenizer(text, cls, lang='en'):\n",
    "    defaults = {\n",
    "        # Flag to check whether to split intents\n",
    "        \"intent_tokenization_flag\": False,\n",
    "        # Symbol on which intent should be split\n",
    "        \"intent_split_symbol\": \"_\",\n",
    "        # text will be tokenized with case sensitive as default\n",
    "        \"case_sensitive\": True,\n",
    "        \"lang\": lang,\n",
    "    }\n",
    "\n",
    "    tok=cls(defaults)    \n",
    "    example = Message(text, {\n",
    "            \"intent\": \"wish\",\n",
    "            \"entities\": []})\n",
    "    # tokenizer\n",
    "    tok.process(example, x='.')\n",
    "    for token in example.get(\"tokens\"):\n",
    "        print(token.text, token.offset)\n",
    "\n",
    "text='text will be tokenized with case sensitive as default'\n",
    "testing_tokenizer(text, WhitespaceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T10:37:37.969367Z",
     "start_time": "2019-11-22T10:37:37.935662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在', '终端', '上', '输出', '单词', '的', '定义', '和', '继承链']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagas.util.rest_common import query_data_by_url\n",
    "lang='zh'\n",
    "sents=\"在终端上输出单词的定义和继承链\"\n",
    "r=query_data_by_url('multilang', 'tokens', {'lang': lang, 'sents': sents})\n",
    "r['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T10:43:04.022125Z",
     "start_time": "2019-11-22T10:43:04.009013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. tokenizer with lang en\n",
      "text 0\n",
      "will 5\n",
      "be 10\n",
      "tokenized 13\n",
      "with 23\n",
      "case 28\n",
      "sensitive 33\n",
      "as 43\n",
      "default 46\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Text\n",
    "from rasa.nlu.tokenizers import Token, Tokenizer\n",
    "from rasa.nlu.constants import (\n",
    "    MESSAGE_RESPONSE_ATTRIBUTE,\n",
    "    MESSAGE_INTENT_ATTRIBUTE,\n",
    "    MESSAGE_TEXT_ATTRIBUTE,\n",
    "    MESSAGE_TOKENS_NAMES,\n",
    "    MESSAGE_ATTRIBUTES,\n",
    "    MESSAGE_SPACY_FEATURES_NAMES,\n",
    "    MESSAGE_VECTOR_FEATURE_NAMES,\n",
    ")\n",
    "\n",
    "class MultilangTokenizer(WhitespaceTokenizer):\n",
    "    def __init__(self, component_config: Dict[Text, Any] = None) -> None:\n",
    "        \"\"\"Construct a new tokenizer using the WhitespaceTokenizer framework.\"\"\"\n",
    "        super().__init__(component_config)\n",
    "        self.lang = self.component_config[\"lang\"]\n",
    "        print(f\".. tokenizer with lang {self.lang}\")\n",
    "        \n",
    "    def tokenize(\n",
    "        self, text: Text, attribute: Text = MESSAGE_TEXT_ATTRIBUTE\n",
    "    ) -> List[Token]:\n",
    "        if self.lang in ('zh','ja'):\n",
    "            r=query_data_by_url('multilang', 'tokens', {'lang': self.lang, 'sents': text})\n",
    "            words=r['data']\n",
    "            running_offset = 0\n",
    "            tokens = []\n",
    "            for word in words:\n",
    "                word_offset = text.index(word, running_offset)\n",
    "                word_len = len(word)\n",
    "                running_offset = word_offset + word_len\n",
    "                tokens.append(Token(word, word_offset))\n",
    "            return tokens\n",
    "        return super().tokenize(text, attribute)\n",
    "        \n",
    "text='text will be tokenized with case sensitive as default'\n",
    "print(testing_tokenizer(text, MultilangTokenizer, 'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T10:42:37.291422Z",
     "start_time": "2019-11-22T10:42:37.265688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. tokenizer with lang zh\n",
      "在 0\n",
      "终端 1\n",
      "上 3\n",
      "输出 4\n",
      "单词 6\n",
      "的 8\n",
      "定义 9\n",
      "和 11\n",
      "继承链 12\n",
      "None\n",
      ".. tokenizer with lang ja\n",
      "望遠 0\n",
      "鏡 2\n",
      "で 3\n",
      "泳いで 4\n",
      "いる 7\n",
      "少女 9\n",
      "を 11\n",
      "見た 12\n",
      "。 14\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text=\"在终端上输出单词的定义和继承链\"\n",
    "print(testing_tokenizer(text, MultilangTokenizer, 'zh'))\n",
    "print(testing_tokenizer(\"望遠鏡で泳いでいる少女を見た。\", MultilangTokenizer, 'ja'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
