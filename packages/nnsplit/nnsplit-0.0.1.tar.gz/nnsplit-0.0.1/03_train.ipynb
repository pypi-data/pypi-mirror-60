{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "from xml.etree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from lxml.etree import iterparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from nnsplit.tokenizer import SoMaJoTokenizer\n",
    "from nnsplit.utils import text_to_id, DATA_DIRECTORY, CUT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "MAX_N_SENTENCES = 100\n",
    "REMOVE_DOT_CHANCE = 0.5\n",
    "LOWERCASE_START_CHANCE = 0.5\n",
    "MIN_LENGTH = 600\n",
    "N_CUTS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def label_paragraph(paragraph, tokenizer):\n",
    "    tokenized_p = tokenizer.split(paragraph)\n",
    "\n",
    "    text = \"\"\n",
    "    labels = []\n",
    "\n",
    "    for sentence in tokenized_p:\n",
    "        for i, token in enumerate(sentence):\n",
    "            whitespace = \" \" if token.space_after else \"\"\n",
    "            text_to_append = token.text + whitespace\n",
    "\n",
    "            if (\n",
    "                token.text == \".\"\n",
    "                and i == len(sentence) - 1\n",
    "                and random.random() < REMOVE_DOT_CHANCE\n",
    "            ):\n",
    "                text_to_append = whitespace\n",
    "                if len(text_to_append) > 0 and len(labels) > 1:\n",
    "                    labels[-2][0] = 0.0\n",
    "\n",
    "            if i == 0 and random.random() < LOWERCASE_START_CHANCE:\n",
    "                text_to_append = token.text.lower() + whitespace\n",
    "\n",
    "            for _ in range(len(text_to_append)):\n",
    "                labels.append([0.0, 0.0])\n",
    "\n",
    "            if len(labels) > 0:\n",
    "                labels[-1][0] = 1.0\n",
    "\n",
    "            text += text_to_append\n",
    "\n",
    "        labels[-1][1] = 1.0\n",
    "\n",
    "    return text, labels\n",
    "\n",
    "def generate_data(paragraph, tokenizer, min_length, n_cuts, cut_length):\n",
    "    if len(paragraph) < min_length:\n",
    "        return [], []\n",
    "\n",
    "    p_text, p_labels = label_paragraph(paragraph, tokenizer)\n",
    "    assert len(p_text) == len(p_labels)\n",
    "\n",
    "    inputs = [[] for _ in range(n_cuts)]\n",
    "    labels = [[] for _ in range(n_cuts)]\n",
    "\n",
    "    for j in range(n_cuts):\n",
    "        start = random.randint(0, len(p_text))\n",
    "\n",
    "        for k in range(cut_length):\n",
    "            if start + k >= len(p_text):\n",
    "                inputs[j].append(0)\n",
    "                labels[j].append([0.0, 0.0])\n",
    "            else:\n",
    "                inputs[j].append(text_to_id(p_text[start + k]))\n",
    "                labels[j].append(p_labels[start + k])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def fast_iter(context):\n",
    "    for event, elem in context:\n",
    "        text = ElementTree.tostring(elem, encoding=\"utf8\").decode(\"utf-8\")\n",
    "        text = re.sub(r\"(<h>(.*?)<\\/h>)\", \"\\n\", text)\n",
    "        text = re.sub(r\"<.*?>\", \"\", text)\n",
    "        yield text\n",
    "\n",
    "        # It's safe to call clear() here because no descendants will be\n",
    "        # accessed\n",
    "        elem.clear()\n",
    "        # Also eliminate now-empty references from the root node to elem\n",
    "        for ancestor in elem.xpath(\"ancestor-or-self::*\"):\n",
    "            while ancestor.getprevious() is not None:\n",
    "                parent = ancestor.getparent()\n",
    "\n",
    "                if parent is not None:\n",
    "                    del parent[0]\n",
    "                else:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_data(\n",
    "    corpus,\n",
    "    language,\n",
    "    data_directory=DATA_DIRECTORY,\n",
    "    max_n_sentences=MAX_N_SENTENCES,\n",
    "    remove_dot_chance=REMOVE_DOT_CHANCE,\n",
    "    lowercase_start_chance=LOWERCASE_START_CHANCE,\n",
    "    min_length=MIN_LENGTH,\n",
    "    n_cuts=N_CUTS,\n",
    "    cut_length=CUT_LENGTH,\n",
    "):\n",
    "    data_directory = Path(data_directory)\n",
    "    data_directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    all_sentences = np.zeros([max_n_sentences, cut_length], dtype=np.int64)\n",
    "    all_labels = np.zeros([max_n_sentences, cut_length, 2], dtype=np.float32)\n",
    "\n",
    "    tokenizer = SoMaJoTokenizer(language)\n",
    "    bar = tqdm(total=max_n_sentences)\n",
    "\n",
    "    i = 0\n",
    "    for paragraph in fast_iter(iterparse(corpus, tag=\"p\")):\n",
    "        text, labels = generate_data(\n",
    "            paragraph, tokenizer, min_length, n_cuts, cut_length\n",
    "        )\n",
    "\n",
    "        length = min(len(text), max_n_sentences - i)\n",
    "\n",
    "        if length > 0:\n",
    "            all_sentences[i : i + length] = text[:length]\n",
    "            all_labels[i : i + length] = labels[:length]\n",
    "\n",
    "        i = i + length\n",
    "\n",
    "        if i == max_n_sentences:\n",
    "            break\n",
    "\n",
    "        bar.update(length)\n",
    "\n",
    "    if i < max_n_sentences:\n",
    "        all_sentences = all_sentences[:i]\n",
    "        all_labels = all_labels[:i]\n",
    "\n",
    "    np.save(data_directory / \"all_sentences.npy\", all_sentences)\n",
    "    np.save(data_directory / \"all_labels.npy\", all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45b7213e33d4d0d90bf9ce15b51d169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prepare_data(\"data/sample-monolingual.xml\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from fastai.train import Learner, DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(127 + 2, 25)\n",
    "        self.lstm1 = nn.LSTM(25, 50, bidirectional=True, batch_first=True, bias=False)\n",
    "        self.lstm2 = nn.LSTM(100, 50, bidirectional=True, batch_first=True, bias=False)\n",
    "        self.out = nn.Linear(100, 2)\n",
    "    \n",
    "    def get_keras_equivalent(self):\n",
    "        k_model = models.Sequential()\n",
    "        k_model.add(layers.Input(shape=(None,)))\n",
    "        \n",
    "        k_model.add(layers.Embedding(127 + 2, 25))\n",
    "        k_model.layers[-1].set_weights([self.embedding.weight.detach().cpu().numpy()])\n",
    "        \n",
    "        k_model.add(layers.Bidirectional(layers.LSTM(50, return_sequences=True, use_bias=False)))\n",
    "        k_model.layers[-1].set_weights([np.transpose(x.detach().cpu().numpy()) for x in self.lstm1.parameters()])\n",
    "                \n",
    "        k_model.add(layers.Bidirectional(layers.LSTM(50, return_sequences=True, use_bias=False)))\n",
    "        k_model.layers[-1].set_weights([np.transpose(x.detach().cpu().numpy()) for x in self.lstm2.parameters()])\n",
    "        \n",
    "        k_model.add(layers.Dense(2))\n",
    "        k_model.layers[-1].set_weights([np.transpose(x.detach().cpu().numpy()) for x in self.out.parameters()])\n",
    "        return k_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.embedding(x.long())\n",
    "        h, _ = self.lstm1(h)\n",
    "        h, _ = self.lstm2(h)\n",
    "        h = self.out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def loss(inputs, targets):\n",
    "    return F.binary_cross_entropy_with_logits(inputs, targets.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_from_tensors(all_sentences, all_labels, valid_percent=0.1, batch_size=128, n_epochs=10):\n",
    "    n_valid = int(len(all_sentences) * valid_percent)\n",
    "\n",
    "    permutation = np.random.permutation(np.arange(len(all_sentences)))\n",
    "    valid_idx, train_idx = permutation[:n_valid], permutation[n_valid:]\n",
    "    \n",
    "    train_dataset = data.TensorDataset(all_sentences[train_idx], all_labels[train_idx])\n",
    "    valid_dataset = data.TensorDataset(all_sentences[valid_idx], all_labels[valid_idx])\n",
    "    \n",
    "    model = Network()\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset,\n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=True,\n",
    "                                   pin_memory=False)\n",
    "    valid_loader = data.DataLoader(valid_dataset, \n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=False,\n",
    "                                   pin_memory=False)\n",
    "\n",
    "    databunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader)\n",
    "    learn = Learner(databunch, model, loss_func=loss)\n",
    "    learn.fit_one_cycle(n_epochs)\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_from_directory(*args, data_directory=DATA_DIRECTORY, **kwargs):\n",
    "    all_sentences = np.load(data_directory / \"all_sentences.npy\")\n",
    "    all_labels = np.load(data_directory / \"all_labels.npy\")\n",
    "    \n",
    "    return train_from_tensors(torch.from_numpy(all_sentences), torch.from_numpy(all_labels), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.707061</td>\n",
       "      <td>0.710403</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706553</td>\n",
       "      <td>0.709086</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.706023</td>\n",
       "      <td>0.707552</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.705439</td>\n",
       "      <td>0.705667</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.704771</td>\n",
       "      <td>0.703313</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.703992</td>\n",
       "      <td>0.700390</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.703075</td>\n",
       "      <td>0.696809</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.701998</td>\n",
       "      <td>0.692478</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.700734</td>\n",
       "      <td>0.687286</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.699259</td>\n",
       "      <td>0.681076</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.697539</td>\n",
       "      <td>0.673614</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.695535</td>\n",
       "      <td>0.664542</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.693191</td>\n",
       "      <td>0.653315</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.690427</td>\n",
       "      <td>0.639100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.687128</td>\n",
       "      <td>0.620628</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.683128</td>\n",
       "      <td>0.595975</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.678175</td>\n",
       "      <td>0.562316</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.671897</td>\n",
       "      <td>0.515954</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.663758</td>\n",
       "      <td>0.453595</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.653062</td>\n",
       "      <td>0.376843</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.639209</td>\n",
       "      <td>0.300076</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.622453</td>\n",
       "      <td>0.248915</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.604535</td>\n",
       "      <td>0.231157</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.587393</td>\n",
       "      <td>0.228190</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.571545</td>\n",
       "      <td>0.227069</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.556734</td>\n",
       "      <td>0.226159</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.542715</td>\n",
       "      <td>0.228556</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.529489</td>\n",
       "      <td>0.236049</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.517228</td>\n",
       "      <td>0.242756</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.505935</td>\n",
       "      <td>0.242583</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.495371</td>\n",
       "      <td>0.237471</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.485356</td>\n",
       "      <td>0.231928</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.475866</td>\n",
       "      <td>0.228227</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.466922</td>\n",
       "      <td>0.226011</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.458491</td>\n",
       "      <td>0.224176</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.450486</td>\n",
       "      <td>0.222307</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.442817</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.435434</td>\n",
       "      <td>0.219202</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.428320</td>\n",
       "      <td>0.217063</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.421455</td>\n",
       "      <td>0.212947</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.414782</td>\n",
       "      <td>0.206707</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.408238</td>\n",
       "      <td>0.199516</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.401791</td>\n",
       "      <td>0.192373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.395421</td>\n",
       "      <td>0.185250</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.389096</td>\n",
       "      <td>0.178001</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.382783</td>\n",
       "      <td>0.170784</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.376470</td>\n",
       "      <td>0.163390</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.370149</td>\n",
       "      <td>0.155599</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.363804</td>\n",
       "      <td>0.147717</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.357430</td>\n",
       "      <td>0.140177</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.351038</td>\n",
       "      <td>0.133096</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.344642</td>\n",
       "      <td>0.126423</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.338251</td>\n",
       "      <td>0.120250</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.331884</td>\n",
       "      <td>0.114647</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.325560</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.319298</td>\n",
       "      <td>0.105149</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.313114</td>\n",
       "      <td>0.101292</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>0.098006</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.301048</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.295190</td>\n",
       "      <td>0.092772</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.289460</td>\n",
       "      <td>0.090643</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.283864</td>\n",
       "      <td>0.088763</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.278403</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.273079</td>\n",
       "      <td>0.085627</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.267893</td>\n",
       "      <td>0.084326</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.262842</td>\n",
       "      <td>0.083178</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.257925</td>\n",
       "      <td>0.082166</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.253141</td>\n",
       "      <td>0.081276</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.248486</td>\n",
       "      <td>0.080494</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.243957</td>\n",
       "      <td>0.079807</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.239551</td>\n",
       "      <td>0.079202</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.235266</td>\n",
       "      <td>0.078669</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.231098</td>\n",
       "      <td>0.078197</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.227044</td>\n",
       "      <td>0.077776</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.077398</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.219265</td>\n",
       "      <td>0.077056</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.215533</td>\n",
       "      <td>0.076746</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.211904</td>\n",
       "      <td>0.076463</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.208373</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.204938</td>\n",
       "      <td>0.075965</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.201596</td>\n",
       "      <td>0.075746</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.198345</td>\n",
       "      <td>0.075547</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.195181</td>\n",
       "      <td>0.075365</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.192102</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.189106</td>\n",
       "      <td>0.075052</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.186191</td>\n",
       "      <td>0.074919</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.183354</td>\n",
       "      <td>0.074802</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.180592</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.177905</td>\n",
       "      <td>0.074611</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.175289</td>\n",
       "      <td>0.074535</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.172743</td>\n",
       "      <td>0.074472</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.170265</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.167853</td>\n",
       "      <td>0.074378</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>0.074346</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.163219</td>\n",
       "      <td>0.074322</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.160994</td>\n",
       "      <td>0.074306</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.158827</td>\n",
       "      <td>0.074295</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.156717</td>\n",
       "      <td>0.074289</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.074286</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.074285</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner = train_from_directory(n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_model(learner, name, data_directory=DATA_DIRECTORY):\n",
    "    data_directory.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # always store on CPU for compatibility, can still convert to CUDA after loading\n",
    "    traced = torch.jit.trace(learner.model.cpu(), learner.data.train_ds[:1][0])\n",
    "    traced.save(str(DATA_DIRECTORY / f\"{name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_model(learner, \"de\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
