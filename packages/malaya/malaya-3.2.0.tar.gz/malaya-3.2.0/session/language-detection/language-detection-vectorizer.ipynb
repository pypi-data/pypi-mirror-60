{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def clean_text(string):\n",
    "    string = re.sub(u'[0-9!@#$%^&*()_\\-+{}|\\~`\\'\";:?/.>,<]', ' ', string.lower(), flags=re.UNICODE)\n",
    "    return re.sub(r'[ ]+', ' ', string.lower()).strip()\n",
    "\n",
    "def simple_textcleaning(string):\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string)\n",
    "    return re.sub(r'[ ]+', ' ', string.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('language-detection-data-v4.json','r') as fopen:\n",
    "    loaded = json.load(fopen)\n",
    "    sentences = [clean_text(text) for text in loaded['text']]\n",
    "    langs = loaded['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ids = [i for i in range(len(langs)) if langs[i] == 'ind']\n",
    "zlm_ids = [i for i in range(len(langs)) if langs[i] == 'zlm']\n",
    "other_ids = [i for i in range(len(langs)) if langs[i] == 'OTHER']\n",
    "eng_ids = [i for i in range(len(langs)) if langs[i] == 'eng']\n",
    "\n",
    "other_sentences = [sentences[i] for i in other_ids]\n",
    "eng_sentences = [sentences[i] for i in eng_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_langs = ['tur', 'rus', 'ita', 'epo', 'deu', 'fra', 'por', 'spa', 'hun',\n",
    "       'heb', 'jpn', 'ukr', 'ber', 'pol', 'fin', 'mkd', 'nld', 'cmn', 'mar',\n",
    "       'dan', 'swe', 'srp', 'lat', 'ara', 'ell', 'ces', 'bul', 'lit', 'toki','kor','fil']\n",
    "\n",
    "multi = []\n",
    "for lang in special_langs:\n",
    "    indices = [i for i in range(len(langs)) if langs[i] == lang]\n",
    "    multi.append([[sentences[i] for i in indices],lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malay-text.txt') as fopen:\n",
    "    malays = filter(None, fopen.read().split('\\n'))\n",
    "\n",
    "with open('indon-text.txt') as fopen:\n",
    "    indons = list(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "with open('00-indonesian-wordlist.txt',encoding='ISO-8859-1') as fopen:\n",
    "    another_indons = list(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "another_indons = [simple_textcleaning(s) for s in another_indons if len(s) > 2]\n",
    "    \n",
    "with open('/home/husein/Malaya/stop-word-kerulnet') as fopen:\n",
    "    stopwords = set(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "ind_set = set(indons)\n",
    "zlm_set = set(malays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for no, i in enumerate(ind_ids):\n",
    "#     if (no+1) % 10000 == 0:\n",
    "#         print('indon %d'%(no + 1))\n",
    "#     sentences[i] = ' '.join(w for w in sentences[i].split() if w in ind_set and w not in stopwords)\n",
    "    \n",
    "# for no, i in enumerate(zlm_ids):\n",
    "#     if (no+1) % 10000 == 0:\n",
    "#         print('malay %d'%(no + 1))\n",
    "#     sentences[i] = ' '.join(w for w in sentences[i].split() if w in zlm_set and w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zlm_sentences = list(filter(None, set([sentences[i] for i in zlm_ids])))\n",
    "# ind_sentences = list(filter(None, set([sentences[i] for i in ind_ids])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tur 10000\n",
      "rus 10000\n",
      "ita 10000\n",
      "epo 10000\n",
      "deu 10000\n",
      "fra 10000\n",
      "por 10000\n",
      "spa 10000\n",
      "hun 10000\n",
      "heb 10000\n",
      "jpn 10000\n",
      "ukr 10000\n",
      "ber 10000\n",
      "pol 10000\n",
      "fin 10000\n",
      "mkd 10000\n",
      "nld 10000\n",
      "cmn 10000\n",
      "mar 10000\n",
      "dan 10000\n",
      "swe 10000\n",
      "srp 10000\n",
      "lat 10000\n",
      "ara 10000\n",
      "ell 10000\n",
      "ces 10000\n",
      "bul 10000\n",
      "lit 10000\n",
      "toki 10000\n",
      "kor 3687\n",
      "fil 0\n"
     ]
    }
   ],
   "source": [
    "sentences = other_sentences + eng_sentences\n",
    "langs = (['OTHER'] * len(other_sentences)) + (['eng'] * len(eng_sentences))\n",
    "for m in multi:\n",
    "    print(m[1],len(m[0]))\n",
    "    sentences += m[0]\n",
    "    langs += [m[1]] * len(m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['OTHER', 'ara', 'ber', 'bul', 'ces', 'cmn', 'dan', 'deu', 'ell',\n",
       "        'eng', 'epo', 'fin', 'fra', 'heb', 'hun', 'ita', 'jpn', 'kor',\n",
       "        'lat', 'lit', 'mar', 'mkd', 'nld', 'pol', 'por', 'rus', 'spa',\n",
       "        'srp', 'swe', 'toki', 'tur', 'ukr'], dtype='<U5'),\n",
       " array([37910, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        50000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,  3687,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(langs,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki-id.txt') as fopen:\n",
    "    id_wiki = [simple_textcleaning(s) for s in list(filter(None, fopen.read().split('\\n')[:60000]))]\n",
    "    \n",
    "with open('wiki-ms.txt') as fopen:\n",
    "    ms_wiki = [simple_textcleaning(s) for s in list(filter(None, fopen.read().split('\\n')[:60000]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indon 10000\n",
      "indon 20000\n",
      "indon 30000\n",
      "indon 40000\n",
      "indon 50000\n",
      "indon 60000\n",
      "malay 10000\n",
      "malay 20000\n",
      "malay 30000\n",
      "malay 40000\n",
      "malay 50000\n",
      "malay 60000\n",
      "CPU times: user 7.29 s, sys: 56 ms, total: 7.35 s\n",
      "Wall time: 7.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for no, i in enumerate(range(len(id_wiki))):\n",
    "    if (no+1) % 10000 == 0:\n",
    "        print('indon %d'%(no + 1))\n",
    "    id_wiki[i] = ' '.join(w for w in id_wiki[i].split() if w in ind_set and w not in stopwords)\n",
    "    \n",
    "for no, i in enumerate(range(len(ms_wiki))):\n",
    "    if (no+1) % 10000 == 0:\n",
    "        print('malay %d'%(no + 1))\n",
    "    ms_wiki[i] = ' '.join(w for w in ms_wiki[i].split() if w in zlm_set and w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_wiki = list(filter(None, set(id_wiki)))\n",
    "ms_wiki = list(filter(None, set(ms_wiki)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences += id_wiki + ms_wiki\n",
    "langs += (['ind'] * len(id_wiki)) + (['zlm'] * len(ms_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['OTHER', 'ara', 'ber', 'bul', 'ces', 'cmn', 'dan', 'deu', 'ell',\n",
       "        'eng', 'epo', 'fin', 'fra', 'heb', 'hun', 'ind', 'ita', 'jpn',\n",
       "        'kor', 'lat', 'lit', 'mar', 'mkd', 'nld', 'pol', 'por', 'rus',\n",
       "        'spa', 'srp', 'swe', 'toki', 'tur', 'ukr', 'zlm'], dtype='<U5'),\n",
       " array([37910, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        50000, 10000, 10000, 10000, 10000, 10000, 57327, 10000, 10000,\n",
       "         3687, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 53692]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(langs,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_chars = CountVectorizer(ngram_range=(3, 5), analyzer='char_wb', max_features=1000000).fit(sentences)\n",
    "delattr(bow_chars, 'stop_words_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('language-detection-data-v5.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'text':sentences,'label':langs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('language-detection-vectorizer.pkl','wb') as fopen:\n",
    "    pickle.dump(bow_chars,fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
